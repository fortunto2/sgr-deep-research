"""–£–ª—å—Ç—Ä–∞-—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ Prefect flows –¥–ª—è batch research –æ–ø–µ—Ä–∞—Ü–∏–π."""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from prefect import flow, task
from prefect.futures import wait

from sgr_deep_research.core.agents.batch_generator_agent import BatchGeneratorAgent
from sgr_deep_research.core.agents import DEFAULT_AGENT
from .research_flow import research_flow

logger = logging.getLogger(__name__)


@task(name="generate-batch-plan")
async def generate_batch_plan_task(
    topic: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Task –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    try:
        logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –¥–ª—è {count} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–º–µ: {topic}")
        
        generator = BatchGeneratorAgent(
            topic=topic,
            count=count,
            languages=languages or ["ru", "en"],
        )
        
        batch_plan = await generator.execute()
        
        return {
            "status": "SUCCESS",
            "topic": batch_plan.topic,
            "total_queries": batch_plan.total_queries,
            "languages": batch_plan.languages,
            "queries": [
                {
                    "id": query.id,
                    "query": query.query,
                    "query_en": query.query_en,
                    "aspect": query.aspect,
                    "scope": query.scope,
                    "suggested_depth": query.suggested_depth,
                }
                for query in batch_plan.queries
            ],
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {e}")
        return {
            "status": "ERROR",
            "error": str(e),
        }


@task(name="save-batch-plan")
def save_batch_plan_task(
    batch_plan: Dict[str, Any],
    batch_name: str,
) -> Path:
    """Task –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    # –î–æ–±–∞–≤–ª—è–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    full_batch_name = f"{batch_name}_{timestamp}"
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è batch
    batch_dir = Path("batches") / full_batch_name
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–ª–∞–Ω - –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
    plan_file = batch_dir / "plan.txt"
    with open(plan_file, "w", encoding="utf-8") as f:
        f.write(f"# Batch: {full_batch_name}\n")
        f.write(f"# Topic: {batch_plan['topic']}\n")
        f.write(f"# Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total queries: {batch_plan['total_queries']}\n\n")
        
        for query in batch_plan["queries"]:
            f.write(f"{query['query']}\n")
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON
    meta_file = batch_dir / "metadata.json"
    with open(meta_file, "w", encoding="utf-8") as f:
        meta = {
            "batch_name": full_batch_name,
            "original_name": batch_name,
            "topic": batch_plan["topic"],
            "created": datetime.now().isoformat(),
            "total_queries": batch_plan["total_queries"],
            "languages": batch_plan["languages"],
            "queries_meta": [
                {
                    "line": i + 1,
                    "query": query["query"],
                    "query_en": query["query_en"],
                    "aspect": query["aspect"],
                    "scope": query["scope"],
                    "suggested_depth": query["suggested_depth"],
                }
                for i, query in enumerate(batch_plan["queries"])
            ],
        }
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    logger.info(f"üìÅ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {batch_dir}")
    return batch_dir


@flow(name="batch-create-flow")
async def batch_create_flow(
    topic: str,
    batch_name: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Path:
    """–£–ª—å—Ç—Ä–∞-—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π Prefect flow –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ batch –ø–ª–∞–Ω–∞ '{batch_name}' –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–ª–∞–Ω
    batch_plan = await generate_batch_plan_task(
        topic=topic,
        count=count,
        languages=languages,
    )
    
    if batch_plan.get("status") != "SUCCESS":
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {batch_plan.get('error')}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω
    batch_dir = save_batch_plan_task(
        batch_plan=batch_plan,
        batch_name=batch_name,
    )
    
    logger.info(f"‚úÖ Batch –ø–ª–∞–Ω —Å–æ–∑–¥–∞–Ω: {batch_dir}")
    return batch_dir


@task(name="save-research-result")  
def save_research_result_task(
    line_num: int,
    query: str,
    agent_type: str,
    suggested_depth: int,
    batch_dir: Path,
    result: Dict[str, Any],
) -> bool:
    """Task –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    try:
        result_dir = batch_dir / f"{line_num:02d}_result"
        result_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        output_file = result_dir / "report.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"# –†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n\n")
            f.write(f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n")
            f.write(f"**–ê–≥–µ–Ω—Ç:** {agent_type}\n\n")
            f.write(f"**–ì–ª—É–±–∏–Ω–∞:** {suggested_depth}\n\n")
            f.write("## –û—Ç–≤–µ—Ç\n\n")
            f.write(result.get("answer", "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ."))
            
            sources = result.get("sources", [])
            if sources:
                f.write("\n\n## –ò—Å—Ç–æ—á–Ω–∏–∫–∏\n\n")
                for source in sources:
                    f.write(f"{source['number']}. [{source['title'] or '–ò—Å—Ç–æ—á–Ω–∏–∫'}]({source['url']})\n")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        meta_file = result_dir / "execution.json"
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({
                "line_number": line_num,
                "query": query,
                "agent_type": agent_type,
                "suggested_depth": suggested_depth,
                "completed_at": datetime.now().isoformat(),
                "status": "COMPLETED" if result.get("status") == "COMPLETED" else "ERROR",
                "output_file": str(output_file),
                "stats": result.get("stats", {}),
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç {line_num} —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ {line_num}: {e}")
        return False


@flow(name="batch-run-flow")
async def batch_run_flow(
    batch_name: str,
    agent_type: str = DEFAULT_AGENT,
    force_restart: bool = False,
    max_concurrent: int = 5,
) -> Dict[str, Any]:
    """
    –£–ª—å—Ç—Ä–∞-—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π Prefect flow –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç flow.submit() –¥–ª—è –∑–∞–ø—É—Å–∫–∞ research subflows –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ!
    """
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è '{batch_name}'")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–ª–∞–Ω
    batch_dir = Path("batches") / batch_name
    if not batch_dir.exists():
        raise FileNotFoundError(f"Batch '{batch_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤: {batch_dir}")
    
    plan_file = batch_dir / "plan.txt"
    meta_file = batch_dir / "metadata.json"
    
    if not plan_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –ø–ª–∞–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –ø–ª–∞–Ω–∞
    queries = []
    with open(plan_file, "r", encoding="utf-8") as f:
        actual_line_num = 0
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line and not line.startswith("#"):
                actual_line_num += 1
                queries.append((actual_line_num, line))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–∏–Ω—ã
    queries_meta = {}
    if meta_file.exists():
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                meta = json.load(f)
                for qmeta in meta.get("queries_meta", []):
                    queries_meta[qmeta["line"]] = qmeta
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    if not queries:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ (–µ—Å–ª–∏ –Ω–µ force_restart)
    queries_to_run = queries
    if not force_restart:
        completed_queries = set()
        for line_num, _ in queries:
            result_dir = batch_dir / f"{line_num:02d}_result"
            exec_file = result_dir / "execution.json"
            if exec_file.exists():
                try:
                    with open(exec_file, "r", encoding="utf-8") as f:
                        exec_data = json.load(f)
                        if exec_data.get("status") == "COMPLETED":
                            completed_queries.add(line_num)
                except:
                    pass
        
        queries_to_run = [(ln, q) for ln, q in queries if ln not in completed_queries]
        
        if completed_queries:
            logger.info(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {len(completed_queries)} —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    if not queries_to_run:
        logger.info("‚úÖ –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
        return {
            "status": "COMPLETED",
            "total_queries": len(queries),
            "executed_queries": 0,
            "skipped_queries": len(queries),
        }
    
    logger.info(f"üìã –ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é: {len(queries_to_run)} –∏–∑ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    logger.info(f"ü§ñ –ê–≥–µ–Ω—Ç: {agent_type}")
    logger.info(f"‚ö° –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö subflows: {max_concurrent}")
    
    # üöÄ –ü–†–û–°–¢–û–ï –†–ï–®–ï–ù–ò–ï - –∏—Å–ø–æ–ª—å–∑—É–µ–º asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö subflows!
    logger.info("‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º research subflows —á–µ—Ä–µ–∑ asyncio.gather...")
    
    import asyncio
    
    async def run_single_research_subflow(line_num: int, query: str, suggested_depth: int):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω research subflow."""
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ research subflow {line_num}: {query[:50]}...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_dir = batch_dir / f"{line_num:02d}_result"
        result_dir.mkdir(exist_ok=True)
        
        try:
            # ‚ö° –ó–ê–ü–£–°–ö–ê–ï–ú RESEARCH –ö–ê–ö SUBFLOW - –ü–†–û–°–¢–û –í–´–ó–´–í–ê–ï–ú!
            result = await research_flow(
                agent_type=agent_type,
                query=query,
                deep_level=suggested_depth,
                output_file=str(result_dir / "report.md"),
                result_dir=str(result_dir),
            )
            
            logger.info(f"‚úÖ Research subflow {line_num} –∑–∞–≤–µ—Ä—à–µ–Ω")
            return line_num, query, suggested_depth, result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ research subflow {line_num}: {e}")
            return line_num, query, suggested_depth, {"status": "ERROR", "error": str(e)}
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—É—Ç–∏–Ω—ã –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤  
    coroutines = []
    for line_num, query in queries_to_run:
        # –ü–æ–ª—É—á–∞–µ–º suggested_depth –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        suggested_depth = queries_meta.get(line_num, {}).get("suggested_depth", 0)
        coro = run_single_research_subflow(line_num, query, suggested_depth)
        coroutines.append(coro)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–º–∞—Ñ–æ—Ä–æ–º
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def run_with_semaphore(coro):
        async with semaphore:
            return await coro
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ subflows –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
    logger.info(f"‚è≥ –í—ã–ø–æ–ª–Ω—è–µ–º {len(coroutines)} research subflows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {max_concurrent}...")
    subflow_results = await asyncio.gather(*[run_with_semaphore(coro) for coro in coroutines])
    
    logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ {len(subflow_results)} research subflows")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_tasks = []
    successful_count = 0
    
    for line_num, query, suggested_depth, result in subflow_results:
        try:
            if result.get("status") == "COMPLETED":
                successful_count += 1
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º task –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            save_task = save_research_result_task.submit(
                line_num=line_num,
                query=query,
                agent_type=agent_type,
                suggested_depth=suggested_depth,
                batch_dir=batch_dir,
                result=result,
            )
            save_tasks.append(save_task)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ subflow {line_num}: {e}")
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if save_tasks:
        wait(save_tasks)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(save_tasks)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    logger.info(f"üéâ Batch '{batch_name}' –∑–∞–≤–µ—Ä—à–µ–Ω!")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful_count}/{len(queries_to_run)}")
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {batch_dir}")
    
    return {
        "status": "COMPLETED",
        "batch_name": batch_name,
        "total_queries": len(queries),
        "executed_queries": len(queries_to_run),
        "successful_queries": successful_count,
        "failed_queries": len(queries_to_run) - successful_count,
        "skipped_queries": len(queries) - len(queries_to_run),
        "batch_dir": str(batch_dir),
    }