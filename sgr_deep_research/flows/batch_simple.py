"""–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π Prefect flow –¥–ª—è batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π."""

import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

from prefect import flow, task
from prefect.artifacts import create_markdown_artifact

from sgr_deep_research.core.agents.batch_generator_agent import BatchGeneratorAgent
from sgr_deep_research.core.agents import DEFAULT_AGENT
from .research_flow import research_flow

logger = logging.getLogger("prefect")


@task(name="generate-batch-queries")
async def generate_batch_queries_task(
    topic: str,
    count: int,
) -> List[Dict[str, Any]]:
    """Task –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {count} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–º–µ: {topic}")

    # –°–æ–∑–¥–∞–µ–º batch generator –∞–≥–µ–Ω—Ç–∞
    generator = BatchGeneratorAgent(topic)
    generator.count = count  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–ª–∞–Ω
    batch_plan = await generator.generate_batch_plan()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ queries –∏–∑ –ø–ª–∞–Ω–∞
    queries = []
    for line in batch_plan.queries:
        queries.append({
            "query": line.query, 
            "suggested_depth": line.suggested_depth, 
            "aspect": getattr(line, 'aspect', ''),
            "scope": getattr(line, 'scope', '')
        })

    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    return queries


@flow(name="batch-simple-flow")
async def batch_simple_flow(
    topic: str,
    count: int = 5,
    agent_type: str = DEFAULT_AGENT,
    max_concurrent: int = 3,
    result_dir: str = "batch_results",
    deep_level: int = 0,
) -> Dict[str, Any]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π batch flow:
    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ task
    2. –ó–∞–ø—É—Å–∫–∞–µ—Ç research subflows –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –±–µ–∑ clarifications
    3. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ markdown –æ—Ç—á–µ—Ç—ã
    """

    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {topic}")
    deep_info = f", deep level {deep_level}" if deep_level > 0 else ""
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {count} –∑–∞–ø—Ä–æ—Å–æ–≤, {max_concurrent} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ{deep_info}")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã
    queries = await generate_batch_queries_task(topic, count)

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    result_path = Path(result_dir)
    result_path.mkdir(parents=True, exist_ok=True)

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    async def run_single_research(query_data: Dict[str, Any], index: int):
        query = query_data["query"]
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π deep_level –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ batch
        actual_depth = deep_level if deep_level > 0 else query_data.get("suggested_depth", 0)

        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        safe_filename = "".join(c for c in query[:50] if c.isalnum() or c in (" ", "-", "_")).rstrip()
        safe_filename = safe_filename.replace(" ", "_")
        output_file = result_path / f"{index:02d}_{safe_filename}.md"

        logger.info(f"üîç –ó–∞–ø—É—Å–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è {index}: {query}")

        try:
            # –í—ã–∑—ã–≤–∞–µ–º research flow –∫–∞–∫ subflow –ë–ï–ó clarifications
            result = await research_flow(
                query=query,
                deep_level=actual_depth,
                output_file=str(output_file),
                result_dir=str(result_path),
                no_clarifications=True,  # –ö–ª—é—á–µ–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä!
            )

            logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ {index}: {query[:30]}...")
            return {
                "index": index,
                "query": query,
                "status": result.get("status", "UNKNOWN"),
                "output_file": str(output_file),
                "result": result,
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ {index}: {e}")
            return {"index": index, "query": query, "status": "ERROR", "error": str(e), "output_file": str(output_file)}

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
    semaphore = asyncio.Semaphore(max_concurrent)

    async def run_with_semaphore(query_data: Dict[str, Any], index: int):
        async with semaphore:
            return await run_single_research(query_data, index)

    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—É—Ç–∏–Ω—ã –¥–ª—è –≤—Å–µ—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
    coroutines = [run_with_semaphore(query_data, i + 1) for i, query_data in enumerate(queries)]

    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ {len(coroutines)} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {max_concurrent}")
    results = await asyncio.gather(*coroutines, return_exceptions=True)

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    completed = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "COMPLETED")
    failed = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "ERROR")
    exceptions = sum(1 for r in results if isinstance(r, Exception))

    logger.info(f"üìä Batch –∑–∞–≤–µ—Ä—à–µ–Ω: {completed} —É—Å–ø–µ—à–Ω–æ, {failed} –æ—à–∏–±–æ–∫, {exceptions} –∏—Å–∫–ª—é—á–µ–Ω–∏–π")

    # –°–æ–∑–¥–∞–µ–º Prefect artifact —Å –æ–±–∑–æ—Ä–æ–º batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±–∑–æ—Ä –¥–ª—è artifact
        artifact_content = f"""# Batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {topic}

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

- **–¢–µ–º–∞:** {topic}
- **–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤:** {len(queries)}
- **–£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ:** {completed}
- **–û—à–∏–±–æ–∫:** {failed}
- **–ò—Å–∫–ª—é—á–µ–Ω–∏–π:** {exceptions}
- **–†–µ–∂–∏–º –≥–ª—É–±–∏–Ω—ã:** {deep_level if deep_level > 0 else "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π"}
- **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ:** `{result_path}`

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º

"""
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        for i, query_data in enumerate(queries, 1):
            query = query_data["query"]
            status = "‚úÖ" if any(isinstance(r, dict) and r.get("index") == i and r.get("status") == "COMPLETED" for r in results) else "‚ùå"
            depth = query_data.get("suggested_depth", 0)
            artifact_content += f"{i}. {status} **{query}** (–≥–ª—É–±–∏–Ω–∞: {depth})\n"

        # –°–æ–∑–¥–∞–µ–º artifact
        create_markdown_artifact(
            key=f"batch-research-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            markdown=artifact_content,
            description=f"Batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {topic}"
        )
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω Prefect artifact —Å –æ–±–∑–æ—Ä–æ–º batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Prefect artifact: {e}")

    return {
        "status": "COMPLETED",
        "topic": topic,
        "total_queries": len(queries),
        "completed": completed,
        "failed": failed,
        "exceptions": exceptions,
        "result_dir": str(result_path),
        "results": [r for r in results if isinstance(r, dict)],
        "queries": queries,
    }
