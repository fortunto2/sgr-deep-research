"""Prefect flow for batch research operations."""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional

from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner
from rich.console import Console

from sgr_deep_research.core.agents.batch_generator_agent import BatchGeneratorAgent
from .research_flow import run_research_agent_task

console = Console()
logger = logging.getLogger(__name__)


@task(name="generate-batch-plan")
async def generate_batch_plan_task(
    topic: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Task –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    try:
        logger.info(f"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –¥–ª—è {count} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–º–µ: {topic}")
        
        generator = BatchGeneratorAgent(
            topic=topic,
            count=count,
            languages=languages or ["ru", "en"],
        )
        
        batch_plan = await generator.execute()
        
        return {
            "status": "SUCCESS",
            "topic": batch_plan.topic,
            "total_queries": batch_plan.total_queries,
            "languages": batch_plan.languages,
            "queries": [
                {
                    "id": query.id,
                    "query": query.query,
                    "query_en": query.query_en,
                    "aspect": query.aspect,
                    "scope": query.scope,
                    "suggested_depth": query.suggested_depth,
                }
                for query in batch_plan.queries
            ]
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {e}")
        import traceback
        logger.error(f"üìú Traceback: {traceback.format_exc()}")
        return {
            "status": "ERROR",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


@task(name="save-batch-plan")
def save_batch_plan_task(
    batch_plan: Dict[str, Any],
    batch_name: str,
) -> Path:
    """Task –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    # –î–æ–±–∞–≤–ª—è–µ–º timestamp –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    full_batch_name = f"{batch_name}_{timestamp}"
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è batch
    batch_dir = Path("batches") / full_batch_name
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–ª–∞–Ω - –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
    plan_file = batch_dir / "plan.txt"
    with open(plan_file, "w", encoding="utf-8") as f:
        f.write(f"# Batch: {full_batch_name}\n")
        f.write(f"# Topic: {batch_plan['topic']}\n")
        f.write(f"# Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total queries: {batch_plan['total_queries']}\n\n")
        
        for query in batch_plan['queries']:
            f.write(f"{query['query']}\n")
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON
    meta_file = batch_dir / "metadata.json"
    with open(meta_file, "w", encoding="utf-8") as f:
        meta = {
            "batch_name": full_batch_name,
            "original_name": batch_name,
            "topic": batch_plan['topic'],
            "created": datetime.now().isoformat(),
            "total_queries": batch_plan['total_queries'],
            "languages": batch_plan['languages'],
            "queries_meta": [
                {
                    "line": i+1,
                    "query": query['query'],
                    "query_en": query['query_en'],
                    "aspect": query['aspect'],
                    "scope": query['scope'],
                    "suggested_depth": query['suggested_depth'],
                }
                for i, query in enumerate(batch_plan['queries'])
            ]
        }
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    logger.info(f"üìÅ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {batch_dir}")
    logger.info(f"üìã –§–∞–π–ª –ø–ª–∞–Ω–∞: {plan_file}")
    logger.info(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {meta_file}")
    
    return batch_dir


@task(name="execute-single-query")
async def execute_single_query_task(
    line_num: int,
    query: str,
    batch_dir: Path,
    agent_type: str,
    suggested_depth: int = 0,
) -> Tuple[int, bool, Dict[str, Any]]:
    """Task –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ batch –ø–ª–∞–Ω–∞."""
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_dir = batch_dir / f"{line_num:02d}_result"
        result_dir.mkdir(exist_ok=True)
        
        output_file = result_dir / "report.md"
        
        logger.info(f"üîÑ –°—Ç—Ä–æ–∫–∞ {line_num}: –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å...")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ research task
        result = await run_research_agent_task(
            agent_type=agent_type,
            query=query,
            deep_level=suggested_depth,
            result_dir=str(result_dir),
        )
        
        if result.get("status") == "COMPLETED":
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"# –†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n\n")
                f.write(f"**–ó–∞–ø—Ä–æ—Å:** {query}\n\n")
                f.write(f"**–ê–≥–µ–Ω—Ç:** {agent_type}\n\n")
                f.write(f"**–ì–ª—É–±–∏–Ω–∞:** {suggested_depth}\n\n")
                f.write("## –û—Ç–≤–µ—Ç\n\n")
                f.write(result.get("answer", ""))
                
                sources = result.get("sources", [])
                if sources:
                    f.write("\n\n## –ò—Å—Ç–æ—á–Ω–∏–∫–∏\n\n")
                    for source in sources:
                        f.write(f"{source['number']}. [{source['title'] or '–ò—Å—Ç–æ—á–Ω–∏–∫'}]({source['url']})\n")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            meta_file = result_dir / "execution.json"
            with open(meta_file, "w", encoding="utf-8") as f:
                json.dump({
                    "line_number": line_num,
                    "query": query,
                    "agent_type": agent_type,
                    "suggested_depth": suggested_depth,
                    "completed_at": datetime.now().isoformat(),
                    "status": "COMPLETED",
                    "output_file": str(output_file),
                    "stats": result.get("stats", {}),
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –°—Ç—Ä–æ–∫–∞ {line_num} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return line_num, True, result
        else:
            logger.error(f"‚ùå –°—Ç—Ä–æ–∫–∞ {line_num} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: {result.get('error', 'Unknown error')}")
            return line_num, False, result
            
    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {e}")
        return line_num, False, {"status": "ERROR", "error": str(e)}


@task(name="load-batch-plan")
def load_batch_plan_task(batch_name: str) -> Tuple[List[Tuple[int, str]], Dict[int, Dict], Path]:
    """Task –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    batch_dir = Path("batches") / batch_name
    
    if not batch_dir.exists():
        raise FileNotFoundError(f"Batch '{batch_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤: {batch_dir}")
    
    plan_file = batch_dir / "plan.txt"
    meta_file = batch_dir / "metadata.json"
    
    if not plan_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –ø–ª–∞–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {plan_file}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–ª–∞–Ω
    queries = []
    with open(plan_file, "r", encoding="utf-8") as f:
        actual_line_num = 0
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line and not line.startswith("#"):
                actual_line_num += 1
                queries.append((actual_line_num, line))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–∏–Ω—ã
    queries_meta = {}
    if meta_file.exists():
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                meta = json.load(f)
                for qmeta in meta.get("queries_meta", []):
                    queries_meta[qmeta["line"]] = qmeta
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    if not queries:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    
    return queries, queries_meta, batch_dir


@task(name="check-completed-queries")
def check_completed_queries_task(
    batch_dir: Path,
    queries: List[Tuple[int, str]],
    force_restart: bool = False,
) -> List[Tuple[int, str]]:
    """Task –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    
    if force_restart:
        return queries
    
    completed_queries = set()
    
    for line_num, _ in queries:
        result_dir = batch_dir / f"{line_num:02d}_result"
        exec_file = result_dir / "execution.json"
        if exec_file.exists():
            try:
                with open(exec_file, "r", encoding="utf-8") as f:
                    exec_data = json.load(f)
                    if exec_data.get("status") == "COMPLETED":
                        completed_queries.add(line_num)
            except:
                pass
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    queries_to_run = [(ln, q) for ln, q in queries if ln not in completed_queries]
    
    if completed_queries:
        logger.info(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {len(completed_queries)} —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    return queries_to_run


@flow(name="batch-create-flow")
async def batch_create_flow(
    topic: str,
    batch_name: str,
    count: int,
    languages: Optional[List[str]] = None,
) -> Path:
    """Prefect flow –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–∞ batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ batch –ø–ª–∞–Ω–∞ '{batch_name}' –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–ª–∞–Ω
    try:
        batch_plan = await generate_batch_plan_task(
            topic=topic,
            count=count,
            languages=languages,
        )
        
        if batch_plan.get("status") != "SUCCESS":
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {batch_plan.get('error')}")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {e}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞: {str(e)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω
    batch_dir = save_batch_plan_task(
        batch_plan=batch_plan,
        batch_name=batch_name,
    )
    
    logger.info(f"‚úÖ Batch –ø–ª–∞–Ω —Å–æ–∑–¥–∞–Ω: {batch_dir}")
    logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {batch_plan['total_queries']}")
    
    return batch_dir


@flow(
    name="batch-run-flow",
    task_runner=ConcurrentTaskRunner(),
)
async def batch_run_flow(
    batch_name: str,
    agent_type: str = "sgr-tools",
    force_restart: bool = False,
    max_concurrent: int = 3,
) -> Dict[str, Any]:
    """Prefect flow –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è batch-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ batch –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è '{batch_name}'")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–ª–∞–Ω
    queries, queries_meta, batch_dir = load_batch_plan_task(batch_name)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ
    queries_to_run = check_completed_queries_task(
        batch_dir=batch_dir,
        queries=queries,
        force_restart=force_restart,
    )
    
    if not queries_to_run:
        logger.info("‚úÖ –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
        return {
            "status": "COMPLETED",
            "total_queries": len(queries),
            "executed_queries": 0,
            "skipped_queries": len(queries),
        }
    
    logger.info(f"üìã –ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é: {len(queries_to_run)} –∏–∑ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    logger.info(f"ü§ñ –ê–≥–µ–Ω—Ç: {agent_type}")
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    tasks = []
    for line_num, query in queries_to_run:
        # –ü–æ–ª—É—á–∞–µ–º suggested_depth –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        suggested_depth = queries_meta.get(line_num, {}).get("suggested_depth", 0)
        
        task = execute_single_query_task.submit(
            line_num=line_num,
            query=query,
            batch_dir=batch_dir,
            agent_type=agent_type,
            suggested_depth=suggested_depth,
        )
        tasks.append(task)
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ (Prefect –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å)
    logger.info(f"‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á...")
    
    results = []
    success_count = 0
    
    for task in tasks:
        line_num, success, result = await task.result()
        results.append((line_num, success, result))
        if success:
            success_count += 1
    
    logger.info(f"üéâ Batch '{batch_name}' –∑–∞–≤–µ—Ä—à–µ–Ω!")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}/{len(tasks)}")
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {batch_dir}")
    
    return {
        "status": "COMPLETED",
        "batch_name": batch_name,
        "total_queries": len(queries),
        "executed_queries": len(tasks),
        "successful_queries": success_count,
        "failed_queries": len(tasks) - success_count,
        "skipped_queries": len(queries) - len(tasks),
        "batch_dir": str(batch_dir),
        "results": results,
    }