"""ExecutionMetrics Pydantic model for job performance tracking.

This module defines the ExecutionMetrics model used for capturing
performance and resource usage statistics during job execution.
"""

from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field, model_validator
from decimal import Decimal, ROUND_HALF_UP
from enum import Enum


class CostBreakdown(BaseModel):
    """Detailed cost breakdown by service."""

    openai_cost: Decimal = Field(
        default=Decimal('0.00'),
        ge=0,
        description="Cost for OpenAI API calls"
    )

    tavily_cost: Decimal = Field(
        default=Decimal('0.00'),
        ge=0,
        description="Cost for Tavily search API calls"
    )

    other_api_costs: Dict[str, Decimal] = Field(
        default_factory=dict,
        description="Costs for other API services"
    )

    compute_cost: Decimal = Field(
        default=Decimal('0.00'),
        ge=0,
        description="Estimated compute/infrastructure cost"
    )

    def get_total_cost(self) -> Decimal:
        """Calculate total cost across all services."""
        total = self.openai_cost + self.tavily_cost + self.compute_cost
        for cost in self.other_api_costs.values():
            total += cost
        return total.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)

    model_config = {
        "json_encoders": {
            Decimal: lambda v: float(v)
        }
    }


class PerformanceMetrics(BaseModel):
    """Performance metrics for execution analysis."""

    avg_response_time_ms: Optional[float] = Field(
        default=None,
        ge=0,
        description="Average API response time in milliseconds"
    )

    cache_hit_rate: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Cache hit rate (0.0-1.0)"
    )

    concurrent_operations: int = Field(
        default=1,
        ge=1,
        description="Peak concurrent operations"
    )

    queue_wait_time_seconds: Optional[float] = Field(
        default=None,
        ge=0,
        description="Time spent waiting in queue"
    )

    execution_efficiency: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Execution efficiency score (0.0-1.0)"
    )


class ExecutionMetrics(BaseModel):
    """Comprehensive execution metrics for research jobs.

    This model captures performance, cost, and resource usage statistics
    for completed research jobs to enable monitoring and optimization.
    """

    # Time metrics
    total_duration_seconds: float = Field(
        ...,
        ge=0,
        description="Total wall clock execution time in seconds"
    )

    queue_time_seconds: float = Field(
        default=0.0,
        ge=0,
        description="Time spent waiting in queue before execution"
    )

    execution_time_seconds: float = Field(
        default=0.0,
        ge=0,
        description="Actual execution time (excluding queue time)"
    )

    # API usage metrics
    api_calls_made: int = Field(
        default=0,
        ge=0,
        description="Total number of external API calls"
    )

    openai_calls: int = Field(
        default=0,
        ge=0,
        description="Number of OpenAI API calls"
    )

    tavily_calls: int = Field(
        default=0,
        ge=0,
        description="Number of Tavily search API calls"
    )

    # Token usage
    tokens_consumed: int = Field(
        default=0,
        ge=0,
        description="Total LLM tokens consumed"
    )

    input_tokens: int = Field(
        default=0,
        ge=0,
        description="Input tokens sent to LLM"
    )

    output_tokens: int = Field(
        default=0,
        ge=0,
        description="Output tokens generated by LLM"
    )

    # Cost metrics
    estimated_cost: Decimal = Field(
        default=Decimal('0.00'),
        ge=0,
        description="Total estimated execution cost in USD"
    )

    cost_breakdown: CostBreakdown = Field(
        default_factory=CostBreakdown,
        description="Detailed cost breakdown by service"
    )

    # Resource usage
    peak_memory_mb: int = Field(
        default=0,
        ge=0,
        description="Peak memory usage in megabytes"
    )

    avg_cpu_percent: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=100.0,
        description="Average CPU utilization percentage"
    )

    disk_io_operations: int = Field(
        default=0,
        ge=0,
        description="Number of disk I/O operations"
    )

    network_bytes_transferred: int = Field(
        default=0,
        ge=0,
        description="Total network bytes transferred"
    )

    # Research-specific metrics
    search_operations: int = Field(
        default=0,
        ge=0,
        description="Number of search operations performed"
    )

    sources_processed: int = Field(
        default=0,
        ge=0,
        description="Number of sources analyzed"
    )

    retry_operations: int = Field(
        default=0,
        ge=0,
        description="Number of retry attempts made"
    )

    # Quality metrics
    research_depth_achieved: int = Field(
        default=0,
        ge=0,
        description="Actual research depth level achieved"
    )

    quality_score: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=10.0,
        description="Research quality score (0-10)"
    )

    completeness_score: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Research completeness score (0.0-1.0)"
    )

    # Performance metrics
    performance: PerformanceMetrics = Field(
        default_factory=PerformanceMetrics,
        description="Detailed performance metrics"
    )

    # Timestamps
    started_at: datetime = Field(
        ...,
        description="Execution start timestamp"
    )

    completed_at: datetime = Field(
        ...,
        description="Execution completion timestamp"
    )

    # Additional metadata
    agent_version: Optional[str] = Field(
        default=None,
        description="Version of the agent used"
    )

    system_info: Dict[str, Any] = Field(
        default_factory=dict,
        description="System information during execution"
    )

    @model_validator(mode='after')
    def validate_model(self):
        """Validate model fields consistency."""
        # Validate execution time doesn't exceed total duration
        if self.execution_time_seconds > self.total_duration_seconds:
            raise ValueError("execution_time_seconds cannot exceed total_duration_seconds")

        # Validate token consumption consistency
        if self.input_tokens > 0 or self.output_tokens > 0:
            expected_total = self.input_tokens + self.output_tokens
            if abs(self.tokens_consumed - expected_total) > 100:  # Allow small discrepancy
                raise ValueError("tokens_consumed should roughly equal input_tokens + output_tokens")

        # Validate completion time is after start time
        if self.started_at and self.completed_at <= self.started_at:
            raise ValueError("completed_at must be after started_at")

        return self

    def calculate_cost_per_token(self) -> Optional[Decimal]:
        """Calculate cost per token if tokens were consumed."""
        if self.tokens_consumed <= 0:
            return None

        cost_per_token = self.estimated_cost / self.tokens_consumed
        return cost_per_token.quantize(Decimal('0.000001'), rounding=ROUND_HALF_UP)

    def calculate_sources_per_minute(self) -> float:
        """Calculate sources processed per minute."""
        if self.execution_time_seconds <= 0:
            return 0.0

        minutes = self.execution_time_seconds / 60
        return round(self.sources_processed / minutes, 2)

    def calculate_efficiency_score(self) -> float:
        """Calculate overall execution efficiency score."""
        scores = []

        # Time efficiency (based on sources per minute)
        sources_per_minute = self.calculate_sources_per_minute()
        time_efficiency = min(1.0, sources_per_minute / 5.0)  # 5 sources/min = 100% efficiency
        scores.append(time_efficiency)

        # Cost efficiency (based on cost per source)
        if self.sources_processed > 0:
            cost_per_source = float(self.estimated_cost) / self.sources_processed
            cost_efficiency = max(0.0, 1.0 - (cost_per_source / 1.0))  # $1/source = 0% efficiency
            scores.append(cost_efficiency)

        # Resource efficiency (based on memory usage)
        if self.peak_memory_mb > 0:
            memory_efficiency = max(0.0, 1.0 - (self.peak_memory_mb / 1000))  # 1GB = 0% efficiency
            scores.append(memory_efficiency)

        # Quality efficiency
        if self.quality_score is not None:
            quality_efficiency = self.quality_score / 10.0
            scores.append(quality_efficiency)

        # Average all efficiency scores
        if scores:
            return round(sum(scores) / len(scores), 3)

        return 0.0

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get a summary of key performance indicators."""
        duration_minutes = self.total_duration_seconds / 60

        summary = {
            "duration_minutes": round(duration_minutes, 2),
            "cost_usd": float(self.estimated_cost),
            "sources_processed": self.sources_processed,
            "api_calls": self.api_calls_made,
            "tokens_used": self.tokens_consumed,
            "efficiency_score": self.calculate_efficiency_score()
        }

        # Add rates if meaningful
        if duration_minutes > 0:
            summary["sources_per_minute"] = round(self.sources_processed / duration_minutes, 1)
            summary["cost_per_minute"] = round(float(self.estimated_cost) / duration_minutes, 3)

        if self.sources_processed > 0:
            summary["cost_per_source"] = round(float(self.estimated_cost) / self.sources_processed, 3)

        return summary

    def add_system_info(self, key: str, value: Any) -> None:
        """Add system information entry."""
        self.system_info[key] = value

    def update_cost_breakdown(self, service: str, cost: Decimal) -> None:
        """Update cost breakdown for a specific service."""
        if service == "openai":
            self.cost_breakdown.openai_cost = cost
        elif service == "tavily":
            self.cost_breakdown.tavily_cost = cost
        elif service == "compute":
            self.cost_breakdown.compute_cost = cost
        else:
            self.cost_breakdown.other_api_costs[service] = cost

        # Update total estimated cost
        self.estimated_cost = self.cost_breakdown.get_total_cost()

    model_config = {
        "validate_assignment": True,
        "json_encoders": {
            datetime: lambda v: v.isoformat(),
            Decimal: lambda v: float(v)
        },
        "json_schema_extra": {
            "example": {
                "total_duration_seconds": 1800.5,
                "queue_time_seconds": 15.2,
                "execution_time_seconds": 1785.3,
                "api_calls_made": 45,
                "openai_calls": 25,
                "tavily_calls": 20,
                "tokens_consumed": 12500,
                "input_tokens": 8000,
                "output_tokens": 4500,
                "estimated_cost": "2.35",
                "cost_breakdown": {
                    "openai_cost": "1.85",
                    "tavily_cost": "0.40",
                    "compute_cost": "0.10"
                },
                "peak_memory_mb": 245,
                "search_operations": 15,
                "sources_processed": 32,
                "retry_operations": 3,
                "research_depth_achieved": 2,
                "quality_score": 8.5,
                "started_at": "2024-01-21T10:30:00Z",
                "completed_at": "2024-01-21T11:00:00Z"
            }
        }
    }